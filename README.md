# 机器学习 

### 1. svm
	a. 核函数包括线性核函数和高斯核函数，在低维平面不可分时用高斯核。
  	b. 损失函数为hingeloss，类似书页；lr的损失函数为loglosss
  	c. svm为非参数模型；lr为参数模型；确定的函数形式会限制模型；但容易解释
  
### 2. pca 通过线性投影把高维数据映射到低维，期望在投影唯独特征方差尽可能大
	具体操作是对所有样本去中心化，计算协方差局长做特征分解，取最大n个特征值对应的向量做投影矩阵

### 3. LR公式：本质是一个线性回归，在结果映射中加了一层sigmoid function（1/1+e^(-z)）

### 4. 给我一些数据集，怎么分类
		特征维数较多-svm
		样本较大-lr
		缺失值较多-决策树

### 5. 决策树如何处理缺失值：
	a. 在选择分裂属性时
		eg使用ID3，有10个样本，第10个样本的a属性缺失。
		那么计算a属性时，用另外9个样本正常计算，然后*0.9
	b. 训练样本时
		根据a划分，但是样本a缺失。
		那么把样本分配到两个子节点，权重变为样本的比例
	c. 测试集分类时
		填充/投票决定

### 6. XGBOOST和GDBT的区别
	a. GDBT是机器学习算法，
		 XGBoost是该算法下的工程实现
	b. 在使用CART作为基分类器时，
		 XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，提高模型的泛化能力
	c. GDBT在模型训练时只使用了代价函数的一阶导数信息，
		 XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶导数和二阶导数
	d. 传统的GDBT采用CART作为基分类器
		 XGBoost支持多种类型的基分类器，比如线性分类器
	e. 传统的GDBT在每轮迭代时使用全部的数据
		 XGBoost类似于随机森林，支持对数据进行采样
	f.  传统的GDBT没有设计对缺失值进行处理
		 XGBoost能够自动学习出缺失值的处理策略

### 7. 特征选择 vs. 特征抽取
	a. 特征选择后的特征是原来特征的一个子集
	b. 特征提取后的新特征是原来特征的映射

### 8. 说一下随机森林
	a. 随机森林是Bagging的一个扩展变体
	b. 随机森林在以决策树作为基学习器构建bagging的基础上，在训练过程中加入了随机属性的选择
	c. 优点：
		随机森林的bootstrap->无偏估计
		能够处理高维数据，而且不需要特征选择
